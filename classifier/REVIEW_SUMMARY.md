# Review Summary: What We've Learned

**Date:** 2026-01-07  
**Reviewed:** 5 out of 20 conversations  
**Sample:** `manual-review-sample-complete.json`

---

## üéØ Key Finding

**The local model (qwen2.5:7b) is misclassifying casual chat conversations as "facilitator" when they should be "affiliative" or "peer".**

---

## üìä What We Reviewed

### Conversations Reviewed (5):

1. **conv-20:** Model says "facilitator" ‚Üí Should be **"affiliative"** (casual peer conversation)
2. **conv-21:** Model says "facilitator" ‚Üí Should be **"affiliative"** (social bonding)
3. **conv-22:** Model says "facilitator" ‚Üí Should be **"affiliative"** (casual sharing)
4. **conv-23:** Model says "facilitator" ‚Üí Should be **"affiliative"** (relationship-focused)
5. **conv-26:** Model says "facilitator" ‚Üí Should be **"affiliative"** (social bonding)

**Pattern:** **100% misclassified** - All 5 conversations that were classified as "facilitator" should actually be "affiliative" or "peer".

---

## üí° Why This Matters

### The Problem:

**Cornell Movie Dialogues conversations are:**
- Casual peer conversations
- Relationship-building/social bonding
- Personal sharing
- **NOT task-oriented**

**Expert vs Facilitator distinction is for:**
- **Expert:** Providing direct answers/explanations (information-seeking)
- **Facilitator:** Guiding discovery with questions (learning/teaching)

**These conversations don't fit either category** - they're social/relational, not informational or educational.

---

## üîç What This Reveals

### The Model's Bias:

**The local model (qwen2.5:7b) is defaulting to "facilitator" for:**
- Casual social chat
- Relationship-building conversations
- Peer conversations
- Any conversation where the AI asks questions (even social questions)

**The model confuses:**
- **Social facilitation** (keeping conversation going, asking "how are you?") 
- **Learning facilitation** (guiding discovery, scaffolding understanding)

---

## üìà Impact on Your Project

### Spatial Positioning Will Be Wrong:

**If these conversations are classified as "facilitator":**
- Y-axis = ~0.25 (bottom) - "emergent/collaborative"
- **Should be:** Y-axis = ~0.6-0.7 (affiliative territory) - "relational/peer"

**Result:** Conversations will cluster in the wrong location in your 3D terrain.

### Pattern Recognition Will Fail:

**Clustering will show:**
- "Facilitator" conversations clustered together
- **But actually:** They're peer/affiliative conversations, not learning facilitation

**Your visualization will encode classification errors, not actual conversational patterns.**

---

## ‚úÖ What This Means for Next Steps

### Option 1: Accept the Limitation (If Using Local Model)

**Document:**
- The local model conflates "social facilitation" with "learning facilitation"
- Casual chat gets misclassified as "facilitator"
- Spatial positioning will be slightly off for relational conversations
- Use for exploratory analysis, not definitive research

### Option 2: Use OpenAI Instead

**Switch to OpenAI:**
```bash
cd classifier
export OPENAI_API_KEY=your-key-here
python3 classifier-openai-fewshot.py \
    conversations-for-classifier.json \
    output-openai.json \
    --few-shot-examples ../few-shot-examples.json \
    --individual \
    --output-dir output/
```

**Test:** Sample 20 OpenAI classifications and see if they handle casual chat better.

### Option 3: Refine Taxonomy

**Add or clarify roles:**
- Distinguish "social facilitator" from "learning facilitator"
- Make "peer" and "affiliative" more prominent
- Provide better examples in few-shot

**Then re-classify with improved taxonomy.**

### Option 4: Filter Dataset

**Only classify task-oriented conversations:**
- Skip casual chat / relationship-building conversations
- Focus on information-seeking, problem-solving, learning conversations
- These are where expert/facilitator distinction matters

---

## üéØ Recommendation

**Based on these 5 reviews:**

1. **The local model has a clear bias** - conflates social and learning facilitation
2. **The dataset might not be ideal** - Cornell Movie Dialogues is mostly casual chat
3. **You need better test cases** - Conversations where expert/facilitator distinction actually matters

**Next steps:**

1. **Review 10 more conversations** to confirm pattern (use `review-helper.py`)
2. **Test OpenAI** on same conversations to see if it's better
3. **Or accept limitation** and document that casual chat gets misclassified

---

## üìù Remaining Reviews

**15 conversations left to review:**
- Review them using `review-helper.py`:
  ```bash
  python3 review-helper.py manual-review-sample-complete.json
  ```
- Or continue manually editing the JSON file

**After reviewing all 20, run:**
```bash
python3 sample-for-review.py manual-review-sample-complete.json --analyze
```

---

## üî¨ What This Tells Us About the Model

**The qwen2.5:7b model:**
- Doesn't distinguish well between social facilitation and learning facilitation
- Defaults to "facilitator" for any question-asking behavior
- Might need better few-shot examples or taxonomy refinement
- May not be sophisticated enough for this classification task

**This is valuable information** - you now know the model's limitations!

