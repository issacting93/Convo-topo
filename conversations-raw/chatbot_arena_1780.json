{
  "id": "chatbot_arena_1780",
  "messages": [
    {
      "role": "user",
      "content": "\n\npoint out  the typora .\n\nAfter Transformer released, pretrained models spurt. GPT\\cite{GPT} and BERT\\cite{BERT} reach SOTA in NLG and NLU tasks.  A typical NLP pipeline is pretraining a big model from scratch or initialed on public corpus as first, then posttrain on custom domain corpus, and finetune downstream task (NLU, NLU and text representation) on labeled dataset in final. Some work try to simplify the pipeline. Unifying NLP tasks as text to text achieves good performance.  The main factor of these progress is increasing model size. Even big models are fewshot or zeroshot learners \\cite{GPT3}. \n\nSome works \\cite{2020arXiv200410964G} show that it is still helpful to tailor a pretrained model to the domain of a target task. \\cite{TLM} uses task data as queries to retrieve a tiny subset of the general corpus and jointly optimizes the task objective and the language modeling objective from scratch.\n\nPrompt \\cite{P3} avoid fine-tuning the big model and use the prompts to utilize pretrained knowledge. Train a large model \\cite{Compress}, then distil the big model to a small model for inference. If a sample is easy enough, Elastic \\t{Elastic} jump out early to save predict time.\n\n\\subsection{Retrieval}\nRetrieval seems a complex system in usual. BM25 \\cite{BM25} is a strong and simple sparse retrieval model. In recent, pretraining representation model and pretraining interaction model are introduced into dense retrieval models \\cite{2021arXiv211113853F}.  Some works dig similar samples, such as Inverse Cloze Task (ICT) \\cite{ICT} and learn representation by contrastive learning  \\cite{Spider}. \n\n\\subsection{Representation}\nThe goal of representation learning \\cite{Representation} is to learn an embedding space in which similar examples stay close to each other while dissimilar ones are far apart \\cite{Dimensionality}. In contrastive learning, the learning procedure is formulated as a classification problem given similar and dissimilar candidates. \n\nContrastive learning can be applied in both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in self-supervised learning. \\cite{CLIP} use in-batch negative samples.  \\cite{ConSERT}, \\cite{SimCSE} provide text argument method.  Some work \\cite{seonwoo2023rankingenhanced}, \\cite{liu2023rankcse} use complex methods capturing a more accurate semantic similarity between similar sentences.\n\n\n"
    },
    {
      "role": "assistant",
      "content": "Typora is not mentioned in the given text. It is a markdown editor and reader that allows users to write and edit markdown files with ease."
    },
    {
      "role": "user",
      "content": "\n\npoint out  the typora .\n\nWe add two tasks in pretraining: LM and contrastive learning. Our total loss \\ref{eqn:loss} includes LM (Language Modeling) loss and CL (Contrastive Learning) loss as below, where α balance the weight between LM Loss and CL Loss.\n\\begin{eqnarray}\n\t\\label{eqn:loss}\n\tTotal\\,Loss = LM\\, Loss + \\alpha \\, CL\\, Loss\n\t% Total\\,Loss = LM\\, Loss + α \\, CL\\, Loss\n\\end{eqnarray}\n\n\n% Fellowing BERT\\cite{BERT}, we use the MLM (Mask Language Modeling) as our LM object. Besides the LM task, we alse optimize the contrasitive learning object with  the Normalized Temperature-scaled Cross-Entropy (NT-Xent) \\cite{SimCLR}. In each batch, we randomly sample N similar text pairs from the corpus while resulting in 2N representations as \\cite{ConSERT}, \\cite{CLIP}. Each sample is trained to find out the similar partner and its counterpart among N(N-1) in-batch negative samples.\n% \\begin{eqnarray}\n% \t% L_{ij} = -log  \\frac{  \\exp(sim(r_{j},r_{j})/ \\tau )  }   { \\sum_{k=1}^{2N} I_{k \\neq i} \\exp( sim(r_{i},r_{k}) / \\tau)}\n% \tL = -log  \\frac{  \\exp(sim(r_{i},r_{j})/ \\tau )  }   { \\sum_{k=1}^{2N} \\exp( sim(r_{i},r_{k}) / \\tau)}\n% \\end{eqnarray}\n% , where sim(·) indicates the cosine similarity function, τ controls the temperature and I is the indicator. \n\nFollowing BERT\\cite{BERT}, we use the MLM (Mask Language Modeling) as our LM object. Besides the LM task, we also optimize the representation learning object in a contrastive learning way. We used the same n-pair / InfoNCE \\cite{InfoNCE} loss as CLIP but with a fixed temperature of 0.1. It is slightly different from the Normalized Temperature-scaled Cross-Entropy (NT-Xent) \\cite{SimCLR} and adaptive to compare embeddings from different sides and costs lower memory.\n\nGiven a batch of N (text, text) pairs, GUR is trained to predict which of the N × N possible (text, text) pairings across a batch actually occurred. Every sample in a batch has one relevant sample and have N-1 irrelevant samples. To do this, GUR learns a text embedding space by jointly training a shared text encoder to maximize the cosine similarity of the text pair embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N*(N-1) incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores.\n\n\n\\cite{2022arXiv220110005N} shows relevance and semantic textual similarity are offensive target for models. To take an example, \"Tom is chasing Jerry\" is relevant to \"Jerry is chasing Tom\". But they are not equal in semantic. For a gene"
    },
    {
      "role": "assistant",
      "content": "The word \"Typora\" is not mentioned in the given text."
    }
  ]
}